# LoveDA Benchmarking
- Ingest the information from this file, implement the Low-Level Tasks and generate the code that satisfies the High and Mid-Level Objectives.

## High Level Overview
So what I want to do is I want to run benchmarks of the LoveDA  Semantic Segmentation dataset, and using a variety of different EO models. I want to then compare the output results from these models. 

## End State Output/ Visualisation

Similar to OpenMM's MMSeg Benchmarks, which use the following (does not need to follwo this exact structure):
Method	Backbone	Crop Size	Lr schd	Mem (GB)	Inf time (fps)	Device	mIoU	mIoU(ms+flip)	config	download. 


## Docs


## High-Level Objectives
- [What do you want to build?]
- Build a set of python scripts that will run benchmarks on the LoveDA dataset, implement these scripts and report and note benchmark results.

## Mid-Level Objectives
- [List of mid-level objectives- what are the steps to achieve the high-level objective?]
- [Each objective should be concrete and measurable]
- [But not to detailed - save details for implementation notes]
- Research potential models for benchmarking.
- Ingest the data.
- Complete the appropriate transformation on the data.
- Keep an up-to-date Readme.md file. 
- push major updates to git.

## Implementation Notes
- [Implortant technical details - what are the imporltant technical detials?]
- [Dependencies and requirements - what are the depencies and requirements?]
- [Coding standards to follow - what are teh doding standaredds to follow?]
- [Other technical guidance]
- use torchvision for segmentation models.
- use torchmetrics for metrics.
- use torchgeo to load in the loveda dataset. it will be downloaded locally already.
- Build a set of class methods that can be utilised across scripts, to minimise repeated code.
- use pytorch lightening to build efficient functions which can be reused in seperate model runs.
- in the readme, keep track tasks completed, yet to complete, brief implementtation quesitons that you might have that will hold back the project until answered.
- iteratively update the readme file after each step.
- At each completion of major steps(prompt fully completed, mid-level objective completed), push changes to git.


## Low-level Tasks
    # Order start to finish
- from the appropriate path
- dependent the dataset and on model specifications unique to each model

## Implementation Details
    # Stream of conciousness walk through everything i want the agent to do and highlight things.
    # Libraries, models, Tools, code repos.
    

## Tasks (Aka Prompts)
    # (tasks or steps to give an engineer to complete this)
[1] Project Overview & Requirements:
Read and summarize the LoveDA Benchmarking spec. List the high-level, mid-level, and low-level objectives along with all dependencies (torchvision, torchmetrics, torchgeo, PyTorch Lightning). Confirm understanding of the expected end-state output (benchmark table with metrics like mIoU, inference time, etc.).

[2] Dataset Ingestion:
Write a Python script that loads the LoveDA dataset using torchgeo from a local path. Ensure error handling for dataset-specific configurations and confirm the dataset is ready for further processing.

[3]Data Transformation Pipeline:
Develop a function that implements required data transformations (e.g., normalization, resizing) for semantic segmentation. Use torchvision transforms and ensure the pipeline is modular for potential adjustments.

[4]Model Initialization & Configuration:
Generate code that instantiates segmentation models from torchvision (e.g., DeepLabV3) with various backbones. Integrate torchmetrics to evaluate performance. Include comments specifying how to swap or extend models for benchmarking.

[5]PyTorch Lightning Module:
Create a PyTorch Lightning module to encapsulate training, validation, and benchmarking workflows. This module should log key metrics and support iterative updates to a central readme file documenting progress and issues.

[6]Benchmarking Script:
Write a script that:

Iterates through selected segmentation models.
Runs inference on the transformed LoveDA dataset.
Measures metrics such as mIoU, memory usage, and inference time.
Outputs results in a structured table format similar to MMSeg Benchmarks.

[7]Utility Functions & Code Reuse:
Develop a set of reusable class methods to handle common tasks (e.g., data loading, metric computation, logging). Ensure these functions reduce code duplication across scripts and are well documented.

[8]Readme & Iterative Documentation:
Implement functionality to update a Readme.md after each completed task. Include sections for tasks completed, pending tasks, and any technical questions or obstacles encountered. After major steps, push the updated documentation to git.



 
